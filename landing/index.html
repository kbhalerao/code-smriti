<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CodeSmriti Technical Brief | AgSci LLC</title>
    <meta name="description" content="CodeSmriti: A semantic code memory system for engineering teams. Technical brief covering architecture, chunking strategies, and performance metrics.">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Serif+4:opsz,wght@8..60,400;8..60,600&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">

    <style>
        :root {
            --bg: #fafaf9;
            --text: #1a1a1a;
            --text-muted: #666;
            --accent: #2563eb;
            --border: #e5e5e5;
            --code-bg: #f5f5f4;
            --serif: 'Source Serif 4', Georgia, 'Times New Roman', serif;
            --mono: 'JetBrains Mono', 'Fira Code', monospace;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            font-size: 18px;
            scroll-behavior: smooth;
        }

        body {
            font-family: var(--serif);
            background: var(--bg);
            color: var(--text);
            line-height: 1.7;
            padding: 2rem 1rem;
        }

        .container {
            max-width: 52rem;
            margin: 0 auto;
        }

        /* Header */
        header {
            border-bottom: 2px solid var(--text);
            padding-bottom: 1.5rem;
            margin-bottom: 2rem;
        }

        .header-top {
            display: flex;
            justify-content: space-between;
            align-items: baseline;
            flex-wrap: wrap;
            gap: 1rem;
        }

        .title {
            font-size: 2rem;
            font-weight: 600;
            letter-spacing: -0.02em;
        }

        .sanskrit {
            font-size: 1rem;
            color: var(--text-muted);
            margin-top: 0.25rem;
        }

        .org {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .meta {
            display: flex;
            gap: 2rem;
            margin-top: 1rem;
            font-size: 0.85rem;
            color: var(--text-muted);
            font-family: var(--mono);
        }

        /* Navigation */
        nav {
            position: sticky;
            top: 0;
            background: var(--bg);
            padding: 0.75rem 0;
            border-bottom: 1px solid var(--border);
            margin-bottom: 2rem;
            z-index: 100;
        }

        nav ul {
            list-style: none;
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem 1.5rem;
            font-size: 0.85rem;
            font-family: var(--mono);
        }

        nav a {
            color: var(--text-muted);
            text-decoration: none;
        }

        nav a:hover {
            color: var(--accent);
        }

        /* Sections */
        section {
            margin-bottom: 3rem;
        }

        h2 {
            font-size: 1.1rem;
            font-weight: 600;
            margin-bottom: 1rem;
            font-family: var(--mono);
            color: var(--text);
        }

        h2::before {
            content: "§ ";
            color: var(--text-muted);
        }

        h3 {
            font-size: 1rem;
            font-weight: 600;
            margin: 1.5rem 0 0.75rem;
        }

        p {
            margin-bottom: 1rem;
        }

        /* Abstract box */
        .abstract {
            background: var(--code-bg);
            border-left: 3px solid var(--text);
            padding: 1.25rem 1.5rem;
            margin-bottom: 2rem;
        }

        .abstract-title {
            font-family: var(--mono);
            font-size: 0.85rem;
            font-weight: 500;
            margin-bottom: 0.5rem;
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        /* Lists */
        ul, ol {
            margin-left: 1.5rem;
            margin-bottom: 1rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.9rem;
        }

        th, td {
            text-align: left;
            padding: 0.75rem 1rem;
            border: 1px solid var(--border);
        }

        th {
            background: var(--code-bg);
            font-family: var(--mono);
            font-weight: 500;
            font-size: 0.85rem;
        }

        td {
            font-family: var(--mono);
            font-size: 0.85rem;
        }

        /* Code blocks */
        pre {
            background: var(--code-bg);
            border: 1px solid var(--border);
            padding: 1rem 1.25rem;
            overflow-x: auto;
            margin: 1rem 0;
            font-family: var(--mono);
            font-size: 0.8rem;
            line-height: 1.5;
        }

        code {
            font-family: var(--mono);
            font-size: 0.85em;
            background: var(--code-bg);
            padding: 0.1em 0.3em;
            border-radius: 2px;
        }

        pre code {
            background: none;
            padding: 0;
        }

        /* ASCII diagrams */
        .diagram {
            background: var(--code-bg);
            border: 1px solid var(--border);
            padding: 1.5rem;
            margin: 1.5rem 0;
            overflow-x: auto;
            font-family: var(--mono);
            font-size: 0.75rem;
            line-height: 1.4;
            white-space: pre;
        }

        /* Links */
        a {
            color: var(--accent);
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        /* Footnotes */
        .footnote {
            font-size: 0.8rem;
            color: var(--text-muted);
            vertical-align: super;
        }

        .references {
            font-size: 0.85rem;
            border-top: 1px solid var(--border);
            padding-top: 1.5rem;
            margin-top: 2rem;
        }

        .references h2::before {
            content: "";
        }

        .references ol {
            margin-left: 1.25rem;
        }

        .references li {
            margin-bottom: 0.75rem;
            color: var(--text-muted);
        }

        /* Footer */
        footer {
            border-top: 2px solid var(--text);
            padding-top: 1.5rem;
            margin-top: 3rem;
            display: flex;
            justify-content: space-between;
            flex-wrap: wrap;
            gap: 1rem;
            font-size: 0.85rem;
            color: var(--text-muted);
        }

        footer a {
            color: var(--text-muted);
        }

        /* Highlight box */
        .highlight {
            background: #fef3c7;
            border: 1px solid #fcd34d;
            padding: 1rem 1.25rem;
            margin: 1.5rem 0;
            font-size: 0.95rem;
        }

        /* Responsive */
        @media (max-width: 640px) {
            html {
                font-size: 16px;
            }

            .header-top {
                flex-direction: column;
            }

            .meta {
                flex-direction: column;
                gap: 0.25rem;
            }

            nav ul {
                gap: 0.25rem 1rem;
            }

            table {
                font-size: 0.8rem;
            }

            th, td {
                padding: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <div class="header-top">
                <div>
                    <h1 class="title">CodeSmriti</h1>
                    <div class="sanskrit">स्मृति — memory, that which is remembered</div>
                </div>
                <div class="org">
                    <a href="https://agsci.com">AgSci LLC</a>
                </div>
            </div>
            <div class="meta">
                <span>Technical Brief v0.4.0</span>
                <span>November 2025</span>
                <span><a href="https://github.com/kbhalerao/code-smriti">github.com/kbhalerao/code-smriti</a></span>
                <span><a href="/docs">API Docs</a></span>
            </div>
        </header>

        <nav>
            <ul>
                <li><a href="#abstract">Abstract</a></li>
                <li><a href="#problem">1. Problem</a></li>
                <li><a href="#architecture">2. Architecture</a></li>
                <li><a href="#chunking">3. Chunking</a></li>
                <li><a href="#schema">4. Schema</a></li>
                <li><a href="#performance">5. Performance</a></li>
                <li><a href="#casestudies">6. Case Studies</a></li>
                <li><a href="#quickstart">7. Quick Start</a></li>
                <li><a href="#references">References</a></li>
            </ul>
        </nav>

        <main>
            <section id="abstract">
                <div class="abstract">
                    <div class="abstract-title">Abstract</div>
                    <p style="margin-bottom: 0;">
                        CodeSmriti is a semantic code memory system that indexes GitHub repositories
                        into a vector database for natural language retrieval. The system employs
                        a <strong>bottom-up aggregation pipeline</strong>: symbols are summarized first, then
                        aggregated into file summaries, folder modules, and finally repository overviews.
                        Hybrid chunking combines tree-sitter AST parsing with LLM-assisted semantic chunking
                        for languages that resist static analysis (Svelte, embedded SQL, templating).
                        Integration with Claude Desktop and VSCode is provided via the Model Context Protocol (MCP).
                        This brief documents the architecture, chunking strategies, and performance characteristics.
                    </p>
                </div>
            </section>

            <section id="problem">
                <h2>1. Problem Statement</h2>
                <p>
                    Engineering teams accumulate solutions across dozens of repositories over years of development.
                    Without institutional memory, organizations face compounding inefficiencies:
                </p>

                <table>
                    <tr>
                        <th>Finding</th>
                        <th>Impact</th>
                        <th>Source</th>
                    </tr>
                    <tr>
                        <td>61% of developers spend &gt;30 min/day searching</td>
                        <td>~100 hours/year per developer</td>
                        <td>[1]</td>
                    </tr>
                    <tr>
                        <td>$47M/year lost per large enterprise</td>
                        <td>Knowledge silos, duplication</td>
                        <td>[2]</td>
                    </tr>
                    <tr>
                        <td>3-9 months to full productivity</td>
                        <td>Slow onboarding</td>
                        <td>[3]</td>
                    </tr>
                    <tr>
                        <td>42% of knowledge unique to individuals</td>
                        <td>Risk when engineers depart</td>
                        <td>[2]</td>
                    </tr>
                </table>

                <p>
                    Traditional code search (grep, GitHub search) operates on keywords, not semantics.
                    Questions like <em>"How did we solve rate limiting?"</em> or <em>"What's our authentication pattern?"</em>
                    require understanding intent, not matching strings.
                </p>
            </section>

            <section id="architecture">
                <h2>2. Architecture</h2>

                <div class="diagram">┌──────────────┐     ┌─────────────────────────────────────┐     ┌──────────────┐
│   GitHub     │────▶│         Ingestion Pipeline          │────▶│  Couchbase   │
│   Repos      │     │                                     │     │  Vector DB   │
└──────────────┘     │  1. Parse files (tree-sitter)       │     └──────┬───────┘
                     │  2. LLM chunk underchunked files    │            │
                     │  3. Summarize symbols → files       │     768-dim embeddings
                     │  4. Aggregate files → modules       │     (nomic-embed-text-v1.5)
                     │  5. Aggregate modules → repo        │            │
                     └─────────────────────────────────────┘            │
                                                                        │
┌───────────────────────────────────────────────────────────────────────┘
│
│              ┌─────────────────────────────────────────────────────────────┐
│              │                     TOOL LAYER (shared)                      │
│              │  list_repos | explore_structure | search_code | get_file    │
│              └─────────────────────────────────────────────────────────────┘
│                       │                                    │
│                       ▼                                    ▼
│              ┌─────────────────────┐          ┌─────────────────────────────┐
│              │  MCP Server         │          │  PydanticAI Agent           │
└─────────────▶│  (rag_mcp_server)   │          │  (pydantic_rag_agent)       │
               │                     │          │                             │
               │  - Direct tool call │          │  - LLM orchestrates tools   │
               │  - Claude reasons   │          │  - ask_codebase() wraps     │
               │  - No LLM needed    │          │  - Local LLM synthesizes    │
               └─────────────────────┘          └─────────────────────────────┘
                        │                                    │
                        ▼                                    ▼
                  Claude Code                         LMStudio / Ollama</div>

                <h3>Components</h3>
                <ul>
                    <li><strong>Ingestion Pipeline:</strong> Bottom-up processor: tree-sitter parsing → LLM chunking → symbol summarization → file aggregation → module aggregation → repo summary</li>
                    <li><strong>Vector Database:</strong> Couchbase with FTS vector search index (768 dimensions)</li>
                    <li><strong>Tool Layer:</strong> Shared implementations for all RAG operations (search, explore, file retrieval)</li>
                    <li><strong>API Server:</strong> FastAPI providing RAG endpoints with hierarchical search (symbol/file/module/repo levels)</li>
                    <li><strong>MCP Server:</strong> Model Context Protocol for Claude Code—direct tool access without intermediate LLM</li>
                    <li><strong>PydanticAI Agent:</strong> LLM-driven mode for local models (LMStudio, Ollama)—LLM orchestrates tool calls</li>
                </ul>

                <h3>Key Technologies</h3>
                <table>
                    <tr>
                        <th>Component</th>
                        <th>Technology</th>
                        <th>Rationale</th>
                    </tr>
                    <tr>
                        <td>Parsing</td>
                        <td>tree-sitter</td>
                        <td>40+ languages, incremental, error-tolerant [4]</td>
                    </tr>
                    <tr>
                        <td>LLM Enrichment</td>
                        <td>Local LLM (Qwen/Llama)</td>
                        <td>Summary generation, semantic chunking, aggregation</td>
                    </tr>
                    <tr>
                        <td>Embeddings</td>
                        <td>nomic-embed-text-v1.5</td>
                        <td>768-dim, 8192 token context, local inference</td>
                    </tr>
                    <tr>
                        <td>Vector DB</td>
                        <td>Couchbase</td>
                        <td>Hybrid FTS + vector, multi-tenant, production-ready</td>
                    </tr>
                    <tr>
                        <td>Integration</td>
                        <td>MCP</td>
                        <td>Standard protocol for LLM tool use</td>
                    </tr>
                </table>
            </section>

            <section id="chunking">
                <h2>3. Chunking Strategy</h2>

                <p>
                    Research confirms that naive chunking methods struggle with code:
                    <em>"Naive chunking methods struggle with accurately delineating meaningful segments of code,
                    leading to issues with boundary definition and the inclusion of irrelevant or incomplete information"</em> [5].
                </p>

                <p>
                    CodeSmriti employs a hybrid approach: AST-aware chunking via tree-sitter as the primary method,
                    with LLM-assisted semantic chunking as a fallback for files that resist static analysis.
                    This has been shown to improve Recall@5 by 4.3 points on code retrieval benchmarks [6].
                </p>

                <h3>Core Principle: Bottom-Up Aggregation</h3>
                <p>
                    Summaries are built from the ground up: individual symbols (functions, classes) are summarized first,
                    then aggregated into file-level summaries, then into folder-based module summaries, and finally
                    into a repository overview. This ensures <strong>every level has meaningful, accurate context</strong>
                    rather than relying on top-down inference.
                </p>

                <h3>Hierarchical Document Types</h3>
                <table>
                    <tr>
                        <th>Document Type</th>
                        <th>Content</th>
                        <th>Purpose</th>
                    </tr>
                    <tr>
                        <td>repo_summary</td>
                        <td>Aggregated from module summaries</td>
                        <td>Repository-level overview and tech stack</td>
                    </tr>
                    <tr>
                        <td>module_summary</td>
                        <td>Aggregated from file summaries</td>
                        <td>Folder/package-level context</td>
                    </tr>
                    <tr>
                        <td>file_index</td>
                        <td>Aggregated from symbol summaries</td>
                        <td>File purpose, key components, imports</td>
                    </tr>
                    <tr>
                        <td>symbol_index</td>
                        <td>LLM summary of function/class</td>
                        <td>Detailed symbol documentation (≥5 lines)</td>
                    </tr>
                </table>

                <h3>Hybrid Chunking: AST + LLM</h3>
                <p>
                    Tree-sitter provides AST-level symbol extraction for most languages.
                    However, some files resist static analysis—Svelte components with complex template logic,
                    SQL embedded in Python strings, or domain-specific patterns.
                </p>

                <p>
                    For these <strong>"underchunked" files</strong> (high line count but few detected symbols),
                    CodeSmriti invokes an LLM semantic chunker to identify logical boundaries:
                </p>

                <pre><code>Primary (tree-sitter):     function_definition, class_definition, method_definition
Fallback (LLM chunker):    SQL queries, business logic blocks, configuration sections,
                           template event handlers, data transformations</code></pre>

                <p>
                    The LLM chunker identifies semantic units like <code>user_subscription_data_fetch</code>
                    or <code>zone_calculation_algorithm</code> that tree-sitter cannot detect, ensuring
                    comprehensive coverage even for unconventional code patterns.
                </p>

                <h3>Junk Filtering</h3>
                <p>Skip files that add noise without value:</p>
                <pre><code>node_modules/    package-lock.json    *.min.js
dist/            yarn.lock            *.min.css
__pycache__/     Cargo.lock           *.map
.git/            poetry.lock          *generated*</code></pre>
            </section>

            <section id="schema">
                <h2>4. Document Schema</h2>

                <div class="highlight">
                    <strong>Key Insight:</strong> Since we have access to the actual repository,
                    we don't need to store code redundantly. Store <em>summaries + line references</em>,
                    fetch code on demand. Content-based hashing ensures deduplication across re-indexes.
                </div>

                <h3>Hierarchical Document Structure</h3>
                <div class="diagram">repo_summary ─────────────────────────────────────────────────────────
│   Aggregated from module summaries. Tech stack, total files/lines.
│   document_id: hash(repo:{repo_id}:{commit})
│
└─ module_summary ────────────────────────────────────────────────────
   │   Folder-based. Aggregated from file summaries.
   │   document_id: hash(module:{repo_id}:{path}:{commit})
   │
   └─ file_index ─────────────────────────────────────────────────────
      │   Aggregated from symbol summaries. Imports, language, all symbols.
      │   document_id: hash(file:{repo_id}:{path}:{commit})
      │
      └─ symbol_index ────────────────────────────────────────────────
            Individual function/class with LLM summary. ≥5 lines only.
            document_id: hash(symbol:{repo_id}:{path}:{name}:{commit})</div>

                <h3>Quality Tracking</h3>
                <p>
                    Each document carries quality metadata indicating its enrichment level:
                </p>

                <table>
                    <tr>
                        <th>Enrichment Level</th>
                        <th>Description</th>
                        <th>Fallback Behavior</th>
                    </tr>
                    <tr>
                        <td><code>llm_summary</code></td>
                        <td>Full LLM-generated summary</td>
                        <td>—</td>
                    </tr>
                    <tr>
                        <td><code>basic</code></td>
                        <td>Docstring + structure only</td>
                        <td>LLM unavailable or timed out</td>
                    </tr>
                    <tr>
                        <td><code>none</code></td>
                        <td>No summary available</td>
                        <td>Parsing failed</td>
                    </tr>
                </table>

                <p>
                    A circuit breaker pattern prevents cascading failures: if LLM calls fail repeatedly,
                    the system gracefully degrades to basic enrichment without blocking ingestion.
                </p>

                <h3>Content-Based Deduplication</h3>
                <p>
                    Document IDs are SHA256 hashes of content keys (repo, path, symbol name, commit).
                    Re-indexing the same commit produces identical IDs, enabling efficient upserts
                    without duplicate detection logic.
                </p>

                <h3>Embedding Strategy</h3>
                <p>
                    Embeddings capture both semantic meaning (from LLM summary) and code patterns
                    (from actual code at index time). For symbols, the embedding combines the summary
                    with a code snippet. This answers both <em>"what does this do?"</em> and
                    <em>"how is it implemented?"</em>
                </p>
            </section>

            <section id="performance">
                <h2>5. Performance</h2>

                <h3>V4 Production Stats (November 2025)</h3>
                <p>Full ingestion of 101 repositories on Mac M3 Ultra with LM Studio (qwen2.5-coder-7b):</p>

                <table>
                    <tr>
                        <th>Metric</th>
                        <th>Value</th>
                        <th>Notes</th>
                    </tr>
                    <tr>
                        <td>Total repositories</td>
                        <td>101</td>
                        <td>99 processed, 2 skipped (pre-existing)</td>
                    </tr>
                    <tr>
                        <td>Documents indexed</td>
                        <td>48,795</td>
                        <td>13K files, 31K symbols, 4K modules</td>
                    </tr>
                    <tr>
                        <td>Total ingestion time</td>
                        <td>32 hours</td>
                        <td>~20 min average per repo</td>
                    </tr>
                    <tr>
                        <td>LLM tokens per file</td>
                        <td>~1,850</td>
                        <td>Estimated input+output</td>
                    </tr>
                </table>

                <h3>Throughput</h3>
                <table>
                    <tr>
                        <th>Operation</th>
                        <th>Performance</th>
                        <th>Configuration</th>
                    </tr>
                    <tr>
                        <td>Embedding generation</td>
                        <td>1,280 docs/min</td>
                        <td>MPS GPU, nomic-embed-text-v1.5</td>
                    </tr>
                    <tr>
                        <td>Search latency</td>
                        <td>&lt;100ms</td>
                        <td>Hybrid vector + FTS</td>
                    </tr>
                    <tr>
                        <td>Average repo ingestion</td>
                        <td>~20 min</td>
                        <td>With LLM enrichment</td>
                    </tr>
                    <tr>
                        <td>Ingestion without LLM</td>
                        <td>1-2 min</td>
                        <td>Basic summaries only</td>
                    </tr>
                </table>

                <h3>Resumable Ingestion</h3>
                <p>
                    The <code>--skip-existing</code> flag enables resumable batch ingestion.
                    Repos with existing V4 documents are skipped, allowing recovery from
                    failures without re-processing completed work.
                </p>
            </section>

            <section id="casestudies">
                <h2>6. Case Studies</h2>

                <p>
                    V4 evaluation suite: 47 questions across 9 categories, tested against 32 proprietary repositories.
                    The evaluation covers symbol lookups, pattern discovery, documentation retrieval, and cross-repo queries.
                </p>

                <h3>Evaluation Results (v0.4.0)</h3>
                <table>
                    <tr>
                        <th>Category</th>
                        <th>Questions</th>
                        <th>Avg Score</th>
                        <th>Notes</th>
                    </tr>
                    <tr>
                        <td>Symbol Pattern</td>
                        <td>3</td>
                        <td>93%</td>
                        <td>Decorators, mixins, base classes</td>
                    </tr>
                    <tr>
                        <td>Symbol Specific</td>
                        <td>5</td>
                        <td>85%</td>
                        <td>Exact function/class lookups</td>
                    </tr>
                    <tr>
                        <td>Svelte Components</td>
                        <td>11</td>
                        <td>78%</td>
                        <td>Template logic, reactive statements</td>
                    </tr>
                    <tr>
                        <td>Code Explanation</td>
                        <td>1</td>
                        <td>80%</td>
                        <td>"How does X work?"</td>
                    </tr>
                    <tr>
                        <td>Documentation</td>
                        <td>8</td>
                        <td>76%</td>
                        <td>RST/MD docs, design principles</td>
                    </tr>
                    <tr>
                        <td>File Pattern</td>
                        <td>7</td>
                        <td>76%</td>
                        <td>Models, views, configuration</td>
                    </tr>
                    <tr>
                        <td>Architecture</td>
                        <td>6</td>
                        <td>76%</td>
                        <td>Multi-tenant, permissions</td>
                    </tr>
                    <tr>
                        <td>Discovery</td>
                        <td>4</td>
                        <td>75%</td>
                        <td>Cross-repo exploration</td>
                    </tr>
                    <tr>
                        <td>Implementation</td>
                        <td>2</td>
                        <td>70%</td>
                        <td>Complex feature deep-dives</td>
                    </tr>
                </table>

                <p><strong>Overall: 47/47 passing (100%), average quality 78%</strong></p>

                <h3>Example Output: Symbol Query</h3>
                <p>Query: <em>"job_counter decorator implementation"</em></p>
                <pre><code># `job_counter` Decorator Implementation

The `job_counter` decorator is implemented in `common/consumer_decorators.py`
and tracks task execution metrics using Redis.

## Key Features
- Tracks success, failure, and pending task counts
- Uses Redis for state management
- Supports both synchronous and asynchronous functions

## Implementation
```python
def job_counter(func: Callable) -> Callable:
    @wraps(func)
    async def async_wrapper(*args: Any, **kwargs: Any) -> Any:
        key_success, key_failed_list, key_pending_list, job_string = _make_keys(func, *args)
        try:
            redis_connection.lpush(key_pending_list, job_string)
            result = await func(*args, **kwargs)
            redis_connection.incr(key_success)
            return result
        except Exception as e:
            redis_connection.lpush(key_failed_list, f"Error: {str(e)}")
            redis_connection.ltrim(key_failed_list, 0, 50)
            raise
        finally:
            redis_connection.lpop(key_pending_list)
```</code></pre>

                <h3>Example Output: Documentation Query</h3>
                <p>Query: <em>"L1 Python best practices"</em></p>
                <pre><code># L1 Python Best Practices

## Naming Conventions
- Use `snake_case` for function and variable names
- Use `PascalCase` for class names
- Constants should be `UPPER_CASE`

## Error Handling
- Use specific exception types rather than bare `except:`
- Always include error messages for debugging
- Use context managers for resource management

## Type Hints
- Always use type hints for function parameters and return values
- Use `typing` module for complex types

```python
def process_user_data(
    user_list: List[Dict[str, str]],
    filter_criteria: Optional[Dict[str, str]] = None
) -> Tuple[Dict[str, List], int]:
    ...
```

Source: `docs/source/developer_guide/audit_standards/L1_python_best_practices.rst`</code></pre>

                <h3>Search Level Selection</h3>
                <p>The LLM automatically selects the appropriate search level based on query intent:</p>
                <ul>
                    <li><code>symbol</code> — "Find the job_counter decorator" → specific function/class</li>
                    <li><code>file</code> — "How do we handle authentication?" → implementation files</li>
                    <li><code>module</code> — "What's in the permissions folder?" → directory overview</li>
                    <li><code>doc</code> — "What are our coding standards?" → RST/MD documentation</li>
                    <li><code>repo</code> — "What repos handle geospatial data?" → cross-repo discovery</li>
                </ul>

                <h3>Strengths</h3>
                <ul>
                    <li>Excellent at finding <strong>code patterns</strong> across repositories</li>
                    <li>Strong at <strong>comparing implementations</strong> (decorators, mixins)</li>
                    <li>Good at synthesizing answers from <strong>scattered sources</strong></li>
                    <li>Documentation queries now use dedicated <code>doc</code> level search</li>
                </ul>
            </section>

            <section id="quickstart">
                <h2>7. Quick Start</h2>

                <pre><code># Clone repository
git clone https://github.com/kbhalerao/code-smriti
cd code-smriti

# Configure environment
cp .env.example .env
# Edit .env with your GitHub token and Couchbase credentials

# Start services
docker-compose up -d

# Verify
curl http://localhost/health</code></pre>

                <h3>MCP Integration (Claude Desktop / Claude Code)</h3>
                <p>Add to <code>~/Library/Application Support/Claude/claude_desktop_config.json</code>:</p>

                <pre><code>{
  "mcpServers": {
    "code-smriti": {
      "command": "uv",
      "args": ["run", "--with", "mcp", "--with", "httpx",
               "path/to/code-smriti/services/mcp-server/rag_mcp_server.py"],
      "env": {
        "CODESMRITI_API_URL": "http://localhost",
        "CODESMRITI_USERNAME": "your-username",
        "CODESMRITI_PASSWORD": "your-password"
      }
    }
  }
}</code></pre>

                <h3>Available Tools</h3>
                <table>
                    <tr>
                        <th>Tool</th>
                        <th>Description</th>
                    </tr>
                    <tr>
                        <td><code>list_repos</code></td>
                        <td>Discover available repositories with document counts</td>
                    </tr>
                    <tr>
                        <td><code>explore_structure</code></td>
                        <td>Navigate directory structure, find key files</td>
                    </tr>
                    <tr>
                        <td><code>search_codebase</code></td>
                        <td>Semantic search with <code>level</code> param: symbol, file, module, repo, or doc</td>
                    </tr>
                    <tr>
                        <td><code>get_file</code></td>
                        <td>Fetch actual code with optional line ranges</td>
                    </tr>
                    <tr>
                        <td><code>ask_codebase</code></td>
                        <td>RAG query with LLM synthesis (calls backend LLM)</td>
                    </tr>
                </table>

                <h3>Two Usage Modes</h3>
                <table>
                    <tr>
                        <th>Mode</th>
                        <th>Use Case</th>
                        <th>How It Works</th>
                    </tr>
                    <tr>
                        <td><strong>MCP Mode</strong></td>
                        <td>Claude Code / Claude Desktop</td>
                        <td>Claude calls tools directly, does its own reasoning and synthesis</td>
                    </tr>
                    <tr>
                        <td><strong>LLM Mode</strong></td>
                        <td>LMStudio / Ollama</td>
                        <td>Local LLM orchestrates tools via PydanticAI agent</td>
                    </tr>
                </table>
            </section>

            <section id="references" class="references">
                <h2>References</h2>
                <ol>
                    <li>
                        Stack Overflow. <em>2024 Developer Survey</em>.
                        <a href="https://survey.stackoverflow.co/2024/">survey.stackoverflow.co/2024</a>
                    </li>
                    <li>
                        Panopto. <em>Workplace Knowledge and Productivity Report</em>. 2018.
                        <a href="https://www.prnewswire.com/news-releases/inefficient-knowledge-sharing-costs-large-businesses-47-million-per-year-300681971.html">PRNewswire</a>
                    </li>
                    <li>
                        Various industry studies on engineering onboarding.
                        <a href="https://hackernoon.com/engineer-onboarding-the-ugly-truth-about-ramp-up-time-7e323t9j">HackerNoon</a>,
                        <a href="https://www.pluralsight.com/product/flow/flow-academy/how-to-improve-onboarding-and-decrease-your-software-engineer-ramp-time">Pluralsight</a>
                    </li>
                    <li>
                        Symflower. <em>Parsing Code with Tree-sitter</em>. 2023.
                        <a href="https://symflower.com/en/company/blog/2023/parsing-code-with-tree-sitter/">symflower.com</a>
                    </li>
                    <li>
                        Qodo. <em>RAG for Large-Scale Code Repos</em>. 2024.
                        <a href="https://www.qodo.ai/blog/rag-for-large-scale-code-repos/">qodo.ai</a>
                    </li>
                    <li>
                        <em>cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree</em>.
                        <a href="https://arxiv.org/html/2506.15655v1">arXiv:2506.15655</a>
                    </li>
                    <li>
                        Weaviate. <em>Evaluation Metrics for Search and Recommendation Systems</em>.
                        <a href="https://weaviate.io/blog/retrieval-evaluation-metrics">weaviate.io</a>
                    </li>
                    <li>
                        Chroma Research. <em>Evaluating Chunking Strategies for Retrieval</em>.
                        <a href="https://research.trychroma.com/evaluating-chunking">research.trychroma.com</a>
                    </li>
                </ol>
            </section>
        </main>

        <footer>
            <div>
                <a href="https://agsci.com">AgSci LLC</a> ·
                <a href="https://github.com/kbhalerao/code-smriti">GitHub</a> ·
                MIT License
            </div>
            <div>
                Last updated: November 2025
            </div>
        </footer>
    </div>
</body>
</html>
