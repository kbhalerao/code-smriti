{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V4 Embedding Space Analysis\n",
    "\n",
    "Structural analysis of code-smriti embeddings:\n",
    "1. Document type separation\n",
    "2. Language clustering\n",
    "3. Hierarchy validation (symbol ‚Üí file ‚Üí module)\n",
    "4. Cross-repo similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Try UMAP if available\n",
    "try:\n",
    "    import umap\n",
    "    HAS_UMAP = True\n",
    "except ImportError:\n",
    "    HAS_UMAP = False\n",
    "    print(\"UMAP not installed. Run: pip install umap-learn\")\n",
    "\n",
    "# Couchbase\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "from storage.couchbase_client import CouchbaseClient\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Embeddings from Couchbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = CouchbaseClient()\n",
    "\n",
    "def fetch_embeddings(doc_type: str, limit: int = 5000) -> pd.DataFrame:\n",
    "    \"\"\"Fetch embeddings for a document type.\"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            META().id as doc_id,\n",
    "            repo_id,\n",
    "            CASE \n",
    "                WHEN type = 'file_index' THEN file_path\n",
    "                WHEN type = 'symbol_index' THEN file_path\n",
    "                WHEN type = 'module_summary' THEN module_path\n",
    "                ELSE repo_id\n",
    "            END as path,\n",
    "            CASE\n",
    "                WHEN type = 'file_index' THEN metadata.language\n",
    "                WHEN type = 'symbol_index' THEN metadata.language\n",
    "                ELSE 'summary'\n",
    "            END as language,\n",
    "            embedding\n",
    "        FROM `code_kosha`\n",
    "        WHERE type = '{doc_type}'\n",
    "          AND embedding IS NOT NULL\n",
    "        LIMIT {limit}\n",
    "    \"\"\"\n",
    "    \n",
    "    results = list(cb.cluster.query(query))\n",
    "    \n",
    "    rows = []\n",
    "    for r in results:\n",
    "        if r.get('embedding'):\n",
    "            rows.append({\n",
    "                'doc_id': r['doc_id'],\n",
    "                'repo_id': r['repo_id'],\n",
    "                'path': r['path'],\n",
    "                'language': r.get('language', 'unknown'),\n",
    "                'type': doc_type,\n",
    "                'embedding': np.array(r['embedding'])\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Fetch all types\n",
    "print(\"Fetching embeddings...\")\n",
    "df_file = fetch_embeddings('file_index', 3000)\n",
    "df_symbol = fetch_embeddings('symbol_index', 3000)\n",
    "df_module = fetch_embeddings('module_summary', 1000)\n",
    "df_repo = fetch_embeddings('repo_summary', 500)\n",
    "\n",
    "print(f\"file_index: {len(df_file)}\")\n",
    "print(f\"symbol_index: {len(df_symbol)}\")\n",
    "print(f\"module_summary: {len(df_module)}\")\n",
    "print(f\"repo_summary: {len(df_repo)}\")\n",
    "\n",
    "# Combine\n",
    "df_all = pd.concat([df_file, df_symbol, df_module, df_repo], ignore_index=True)\n",
    "print(f\"\\nTotal: {len(df_all)} documents with embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embedding matrix\n",
    "embeddings = np.vstack(df_all['embedding'].values)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"Embedding dim: {embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA Analysis - Variance Explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA to understand dimensionality\n",
    "pca_full = PCA(n_components=min(100, embeddings.shape[1]))\n",
    "pca_full.fit(embeddings)\n",
    "\n",
    "# Plot variance explained\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cumulative variance\n",
    "cumvar = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "axes[0].plot(cumvar, 'b-', linewidth=2)\n",
    "axes[0].axhline(y=0.9, color='r', linestyle='--', label='90% variance')\n",
    "axes[0].axhline(y=0.95, color='orange', linestyle='--', label='95% variance')\n",
    "axes[0].set_xlabel('Number of Components')\n",
    "axes[0].set_ylabel('Cumulative Explained Variance')\n",
    "axes[0].set_title('PCA: Cumulative Variance Explained')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Find 90% and 95% thresholds\n",
    "n_90 = np.argmax(cumvar >= 0.9) + 1\n",
    "n_95 = np.argmax(cumvar >= 0.95) + 1\n",
    "print(f\"Components for 90% variance: {n_90}\")\n",
    "print(f\"Components for 95% variance: {n_95}\")\n",
    "\n",
    "# Individual variance (first 30)\n",
    "axes[1].bar(range(30), pca_full.explained_variance_ratio_[:30])\n",
    "axes[1].set_xlabel('Component')\n",
    "axes[1].set_ylabel('Explained Variance Ratio')\n",
    "axes[1].set_title('PCA: Variance per Component (first 30)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Type Separation (2D Visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA to 2D for visualization\n",
    "pca_2d = PCA(n_components=2)\n",
    "embeddings_2d = pca_2d.fit_transform(embeddings)\n",
    "\n",
    "df_all['pca_x'] = embeddings_2d[:, 0]\n",
    "df_all['pca_y'] = embeddings_2d[:, 1]\n",
    "\n",
    "# Plot by document type\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "type_colors = {\n",
    "    'symbol_index': 'blue',\n",
    "    'file_index': 'green', \n",
    "    'module_summary': 'orange',\n",
    "    'repo_summary': 'red'\n",
    "}\n",
    "\n",
    "for doc_type, color in type_colors.items():\n",
    "    mask = df_all['type'] == doc_type\n",
    "    ax.scatter(\n",
    "        df_all.loc[mask, 'pca_x'],\n",
    "        df_all.loc[mask, 'pca_y'],\n",
    "        c=color, label=doc_type, alpha=0.5, s=20\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%} var)')\n",
    "ax.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%} var)')\n",
    "ax.set_title('Document Type Separation (PCA)')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"PC1 + PC2 explain {sum(pca_2d.explained_variance_ratio_):.1%} of variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP visualization (better for cluster structure)\n",
    "if HAS_UMAP:\n",
    "    print(\"Running UMAP (this may take a minute)...\")\n",
    "    reducer = umap.UMAP(n_components=2, n_neighbors=30, min_dist=0.1, random_state=42)\n",
    "    embeddings_umap = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    df_all['umap_x'] = embeddings_umap[:, 0]\n",
    "    df_all['umap_y'] = embeddings_umap[:, 1]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    for doc_type, color in type_colors.items():\n",
    "        mask = df_all['type'] == doc_type\n",
    "        ax.scatter(\n",
    "            df_all.loc[mask, 'umap_x'],\n",
    "            df_all.loc[mask, 'umap_y'],\n",
    "            c=color, label=doc_type, alpha=0.5, s=20\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('UMAP 1')\n",
    "    ax.set_ylabel('UMAP 2')\n",
    "    ax.set_title('Document Type Separation (UMAP)')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping UMAP (not installed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Language Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to file_index and symbol_index (have language info)\n",
    "df_code = df_all[df_all['type'].isin(['file_index', 'symbol_index'])].copy()\n",
    "\n",
    "# Top languages\n",
    "top_langs = df_code['language'].value_counts().head(8).index.tolist()\n",
    "df_code_top = df_code[df_code['language'].isin(top_langs)]\n",
    "\n",
    "print(f\"Top languages: {top_langs}\")\n",
    "print(f\"Documents: {len(df_code_top)}\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "lang_colors = plt.cm.tab10(np.linspace(0, 1, len(top_langs)))\n",
    "\n",
    "for i, lang in enumerate(top_langs):\n",
    "    mask = df_code_top['language'] == lang\n",
    "    ax.scatter(\n",
    "        df_code_top.loc[mask, 'pca_x'],\n",
    "        df_code_top.loc[mask, 'pca_y'],\n",
    "        c=[lang_colors[i]], label=lang, alpha=0.5, s=20\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('Language Clustering (PCA)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP by language\n",
    "if HAS_UMAP and 'umap_x' in df_all.columns:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    for i, lang in enumerate(top_langs):\n",
    "        mask = df_code_top['language'] == lang\n",
    "        ax.scatter(\n",
    "            df_code_top.loc[mask, 'umap_x'],\n",
    "            df_code_top.loc[mask, 'umap_y'],\n",
    "            c=[lang_colors[i]], label=lang, alpha=0.5, s=20\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('UMAP 1')\n",
    "    ax.set_ylabel('UMAP 2')\n",
    "    ax.set_title('Language Clustering (UMAP)')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Intra-Repo vs Inter-Repo Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample repos with enough documents\n",
    "repo_counts = df_file['repo_id'].value_counts()\n",
    "sample_repos = repo_counts[repo_counts >= 20].head(10).index.tolist()\n",
    "\n",
    "print(f\"Analyzing {len(sample_repos)} repos with 20+ files each\")\n",
    "\n",
    "intra_sims = []  # Similarities within same repo\n",
    "inter_sims = []  # Similarities across repos\n",
    "\n",
    "for repo in sample_repos:\n",
    "    repo_mask = df_file['repo_id'] == repo\n",
    "    repo_embeds = np.vstack(df_file.loc[repo_mask, 'embedding'].values)\n",
    "    \n",
    "    # Intra-repo: pairwise within repo (sample to limit computation)\n",
    "    if len(repo_embeds) > 50:\n",
    "        idx = np.random.choice(len(repo_embeds), 50, replace=False)\n",
    "        repo_embeds_sample = repo_embeds[idx]\n",
    "    else:\n",
    "        repo_embeds_sample = repo_embeds\n",
    "    \n",
    "    sim_matrix = cosine_similarity(repo_embeds_sample)\n",
    "    # Get upper triangle (excluding diagonal)\n",
    "    triu_idx = np.triu_indices(len(sim_matrix), k=1)\n",
    "    intra_sims.extend(sim_matrix[triu_idx].tolist())\n",
    "\n",
    "# Inter-repo: compare files from different repos\n",
    "for i, repo1 in enumerate(sample_repos[:5]):\n",
    "    for repo2 in sample_repos[i+1:6]:\n",
    "        embeds1 = np.vstack(df_file.loc[df_file['repo_id'] == repo1, 'embedding'].values[:20])\n",
    "        embeds2 = np.vstack(df_file.loc[df_file['repo_id'] == repo2, 'embedding'].values[:20])\n",
    "        \n",
    "        cross_sim = cosine_similarity(embeds1, embeds2)\n",
    "        inter_sims.extend(cross_sim.flatten().tolist())\n",
    "\n",
    "print(f\"Intra-repo pairs: {len(intra_sims)}\")\n",
    "print(f\"Inter-repo pairs: {len(inter_sims)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.hist(intra_sims, bins=50, alpha=0.7, label=f'Intra-repo (mean={np.mean(intra_sims):.3f})', density=True)\n",
    "ax.hist(inter_sims, bins=50, alpha=0.7, label=f'Inter-repo (mean={np.mean(inter_sims):.3f})', density=True)\n",
    "\n",
    "ax.set_xlabel('Cosine Similarity')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('File Similarity: Within Repo vs Across Repos')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nIntra-repo similarity: {np.mean(intra_sims):.3f} ¬± {np.std(intra_sims):.3f}\")\n",
    "print(f\"Inter-repo similarity: {np.mean(inter_sims):.3f} ¬± {np.std(inter_sims):.3f}\")\n",
    "print(f\"Separation: {np.mean(intra_sims) - np.mean(inter_sims):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hierarchy Validation (Symbol ‚Üí File ‚Üí Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if symbols are closer to their parent file than to random files\n",
    "\n",
    "# Build file embedding lookup\n",
    "file_embeds = {row['path']: row['embedding'] for _, row in df_file.iterrows()}\n",
    "\n",
    "# For each symbol, compute similarity to its file vs random files\n",
    "symbol_to_own_file = []\n",
    "symbol_to_random_file = []\n",
    "\n",
    "sample_symbols = df_symbol.sample(min(500, len(df_symbol)))\n",
    "random_files = list(file_embeds.values())\n",
    "\n",
    "for _, sym in sample_symbols.iterrows():\n",
    "    sym_embed = sym['embedding'].reshape(1, -1)\n",
    "    sym_file = sym['path']\n",
    "    \n",
    "    # Similarity to own file\n",
    "    if sym_file in file_embeds:\n",
    "        own_file_embed = file_embeds[sym_file].reshape(1, -1)\n",
    "        own_sim = cosine_similarity(sym_embed, own_file_embed)[0, 0]\n",
    "        symbol_to_own_file.append(own_sim)\n",
    "        \n",
    "        # Similarity to random file\n",
    "        rand_idx = np.random.randint(0, len(random_files))\n",
    "        rand_embed = random_files[rand_idx].reshape(1, -1)\n",
    "        rand_sim = cosine_similarity(sym_embed, rand_embed)[0, 0]\n",
    "        symbol_to_random_file.append(rand_sim)\n",
    "\n",
    "print(f\"Computed {len(symbol_to_own_file)} symbol-file pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.hist(symbol_to_own_file, bins=50, alpha=0.7, \n",
    "        label=f'Symbol ‚Üí Own File (mean={np.mean(symbol_to_own_file):.3f})', density=True)\n",
    "ax.hist(symbol_to_random_file, bins=50, alpha=0.7, \n",
    "        label=f'Symbol ‚Üí Random File (mean={np.mean(symbol_to_random_file):.3f})', density=True)\n",
    "\n",
    "ax.set_xlabel('Cosine Similarity')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Hierarchy Validation: Symbol to File Similarity')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSymbol ‚Üí Own file: {np.mean(symbol_to_own_file):.3f} ¬± {np.std(symbol_to_own_file):.3f}\")\n",
    "print(f\"Symbol ‚Üí Random file: {np.mean(symbol_to_random_file):.3f} ¬± {np.std(symbol_to_random_file):.3f}\")\n",
    "print(f\"Hierarchy coherence: {np.mean(symbol_to_own_file) - np.mean(symbol_to_random_file):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Repo Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average embedding per repo (centroid)\n",
    "repo_centroids = {}\n",
    "\n",
    "for repo in df_file['repo_id'].unique():\n",
    "    repo_embeds = np.vstack(df_file.loc[df_file['repo_id'] == repo, 'embedding'].values)\n",
    "    repo_centroids[repo] = repo_embeds.mean(axis=0)\n",
    "\n",
    "# Select top repos by file count for visualization\n",
    "top_repos = repo_counts.head(15).index.tolist()\n",
    "centroid_matrix = np.vstack([repo_centroids[r] for r in top_repos])\n",
    "\n",
    "# Compute similarity matrix\n",
    "repo_sim_matrix = cosine_similarity(centroid_matrix)\n",
    "\n",
    "# Shorten repo names for display\n",
    "short_names = [r.split('/')[-1][:15] for r in top_repos]\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(repo_sim_matrix, xticklabels=short_names, yticklabels=short_names,\n",
    "            cmap='RdYlBu_r', center=0.5, annot=True, fmt='.2f', ax=ax)\n",
    "ax.set_title('Repository Similarity Matrix (File Centroids)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EMBEDDING ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Data Overview:\")\n",
    "print(f\"   Total documents: {len(df_all):,}\")\n",
    "print(f\"   Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"   Unique repos: {df_all['repo_id'].nunique()}\")\n",
    "\n",
    "print(f\"\\nüìà PCA Analysis:\")\n",
    "print(f\"   Components for 90% variance: {n_90}\")\n",
    "print(f\"   Components for 95% variance: {n_95}\")\n",
    "print(f\"   First 2 PCs explain: {sum(pca_2d.explained_variance_ratio_):.1%}\")\n",
    "\n",
    "print(f\"\\nüîó Similarity Analysis:\")\n",
    "print(f\"   Intra-repo file similarity: {np.mean(intra_sims):.3f}\")\n",
    "print(f\"   Inter-repo file similarity: {np.mean(inter_sims):.3f}\")\n",
    "print(f\"   Repo coherence (diff): {np.mean(intra_sims) - np.mean(inter_sims):.3f}\")\n",
    "\n",
    "print(f\"\\nüèóÔ∏è Hierarchy Validation:\")\n",
    "print(f\"   Symbol ‚Üí Own file: {np.mean(symbol_to_own_file):.3f}\")\n",
    "print(f\"   Symbol ‚Üí Random file: {np.mean(symbol_to_random_file):.3f}\")\n",
    "print(f\"   Hierarchy coherence: {np.mean(symbol_to_own_file) - np.mean(symbol_to_random_file):.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
